---
title: "Blog Post 2"
---

In the beginning of the chapter, the authors talked about Serena Williams and her journey with pregnancy. After her pregnancy, Williams talked about the complications of her daughter’s childbirth and that led to a national conversation about the disparities in mortality rates of women during childbirth. Because of Williams, many people noticed the lack of data that is collected on maternal mortality and how there is no national system for tracking complications sustained in pregnancy and childbirth. If it was not for Williams and organizations like Sistersong and R.O.S.E, there would not have been dialogue about making changes made to that system which is problematic because ignoring issues like this prevents people from seeing the disparities in mortality rates and how that connects to larger systematic problems.

One of the main takeaways from this chapter is how power dynamics affects data science. A quote by Robert M. Young that stuck out to me was, “A racist society will give you a racist science”. Although AI itself does not hold racist, sexist, or classist bias, the people that created these systems might hold these beliefs and that is how these technologies perpetuate the bias that exist in our society. Also, an overwhelming number of people that work in tech fields are white and/or male. The people that benefit the most from our current socioeconomic system are white and male. People in positions of power rarely recognize instances of oppression and the authors argue that could be the case why most of the bias in AI and data collection goes undetected. This would make sense people who are experiencing oppression are the ones fighting to make changes to our system. If it wasn’t for Williams, the racial disparities in childbirth would not have been noticed. 

This article also mentions Joy Buolamwini and Timnit Gebru and their work towards diversifying facial recognition data. Although there are benefits to having data that is more inclusive and representative of the population, there could be negative effects like how facial recognition software has increased police surveillance in impoverished communities and how it was used to track people during the Black Lives Matter protests. Timnit Gebru recently got fired from her position at Google for writing a paper that voiced concerns about the ethics behind some of Google’s language models. Although Gebru worked as a part of Google’s AI ethics team, she was let go because her paper went against Google’s business practices. After her firing garnered some attention, Google decided to change its diversity and research policies. This situation is interesting because it shows how difficult it is to make changes in this field when you are not in a position of power. It also shows how powerful people can be if they came together to fight for lasting change. 
